{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_tp_Kakhran_Henry.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClX7gdWGv7b7",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "This practical work is about creation of elementary models in keras.\n",
        "While it focuses on image classification, it illustrates many important functionalities of the keras framework and tries to provide the user with good deep-learning development strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lidilM2ixDZK",
        "colab_type": "text"
      },
      "source": [
        "## Setup\n",
        "Before you start, please mount your google drive.\n",
        "For each exercise, create an output directory to store monitoring data and your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzPwG_5Y9DN8",
        "colab_type": "code",
        "outputId": "80c5d512-b546-48c4-d90b-95072f1c591e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg1t3mosYluc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "from keras.datasets import mnist, fashion_mnist, cifar10\n",
        "from keras import metrics, utils\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from numpy import expand_dims"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlHjnYHB455x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtL8KNsJxxGa",
        "colab_type": "text"
      },
      "source": [
        "# EX1: Lenet-like classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m0voN8YyLhD",
        "colab_type": "text"
      },
      "source": [
        "## 1- Build model architecture\n",
        "The architecture has to be a CLASS called \"Lenet_like\".\n",
        "\n",
        "- I can define the width: number of filters in the first layer.\n",
        "- I can define the depth: number of conv layers.\n",
        "- I can specify the number of classes: output neurons.\n",
        "\n",
        "Lenet_like has no input tensor. But it has a \\_\\_call\\_\\_ function and can therefore be called on an input later.\n",
        "\n",
        "The rule to go from depth d to depth d+1, is to reduce the spatial size by a factor of 2 in each direction.\n",
        "\n",
        "Hidden dense layer will have 512 units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ-tLkW79EWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Lenet_like:\n",
        "    # Initialisation\n",
        "  def _init_(self, width, depth, output):\n",
        "    self.width = width            # number of filters in the first layer \n",
        "    self.depth = depth            # number of conv layers\n",
        "    self.output = output          # number of classes\n",
        "\n",
        "  def  __call__(self, x_):\n",
        "      # input layer: relu activation function\n",
        "    y_ = Conv2D(filters = 32, padding = 'same', activation = 'relu')(x_) \n",
        "    \n",
        "      # hidden layers: reducion spatial size by 2\n",
        "    y_ = (Conv2D(32, (3, 3), padding = \"same\", activation = \"relu\"))(y_)\n",
        "    y_ = (MaxPooling2D(pool_size=(2, 2)))(y_)\n",
        "    \n",
        "    y_ = (Conv2D(64, (3, 3), padding = \"same\", activation = \"relu\"))(y_)\n",
        "    y_ = (MaxPooling2D(pool_size=(2, 2)))(y_)\n",
        "   \n",
        "    y_ = (Conv2D(128, (3, 3), padding = \"same\", activation = \"relu\"))(y_)\n",
        "    y_ = (MaxPooling2D(pool_size=(2, 2)))(y_)\n",
        "   \n",
        "    y_ = (Conv2D(256, (3, 3), padding = \"same\", activation = \"relu\"))(y_)\n",
        "    y_ = (MaxPooling2D(pool_size = (2, 2)))(y_)\n",
        "    \n",
        "    y_ = (Conv2D(512, (3, 3), padding = \"same\", activation = \"relu\"))(y_)\n",
        "    y_ = (MaxPooling2D(pool_size = (2, 2)))(y_)\n",
        "    \n",
        "      # output layer: softmax activation function instead of relu\n",
        "    y_=(Dense(512, pandding = 'same', activation = 'softmax'))    \n",
        "    \n",
        "    return y_\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVKIV9Yhz1w2",
        "colab_type": "text"
      },
      "source": [
        "## 2- A function to create a model with Lenet_like architecture\n",
        "I want the model to be able to fit on the following datasets:\n",
        "\n",
        "- mnist\n",
        "- fashion-mnist[texte du lien](https://)\n",
        "- cifar-10\n",
        "\n",
        "Create a function called \"make_lenet_model\" that take the name of one of these dataset as a str.\n",
        "It returns a keras Model object.\n",
        "It should obviously take all arguments to init the Lenet_like architecture.\n",
        "Arguments other than the dataset might have default values.\n",
        "I want to be able to monitor the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yfwER7cof8G",
        "colab_type": "text"
      },
      "source": [
        "### About the datasets:\n",
        "\n",
        "#### MNIST:  \n",
        "- images of handwritten numbers\n",
        "- 70 000 images (60 000/10 000)\n",
        "- size 28x28 (width = height = 28)\n",
        "- grayscale (depth = 1)\n",
        "- 10 classes (0 to 9)\n",
        "\n",
        "#### FASHION-MNIST\n",
        "- images of clothes\n",
        "- 70 000 images (60 000/10 000)\n",
        "- size 28x28\n",
        "- grayscale (depth = 1)\n",
        "- 10 classes (ex:T-shirt, trousers, dress...)\n",
        "\n",
        "#### CIFAR-10\n",
        "- photos of objects\n",
        "- 60 000 images (50 000/10 000)\n",
        "- size 32x32\n",
        "- color (depth=3)\n",
        "- 10 classes (ex: airplanes, automobile, bird, cat...)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYiDa77m9FJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_lenet_model(dataset, output=10):\n",
        "  (x_train, y_train), (x_test, y_test) = dataset.load_data()\n",
        "\n",
        "  if (dataset == \"mnist\" or dataset == \"fashion_mnist\"):\n",
        "    width = 28       \n",
        "    depth = 1\n",
        "\n",
        "  elif (dataset == \"cifar10\"):\n",
        "    width = 32   \n",
        "    depth=3\n",
        "      # reshape to 28x28 and to grayscale\n",
        "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "   # convert from integers to floats\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "\n",
        "    # normalize to range 0-1\n",
        "  x_train = x_train / 255.0\n",
        "  x_test = x_test / 255.0\n",
        "\n",
        "    # one hot encode target values\n",
        "  y_train = to_categorical(y_train)\n",
        "  y_test = to_categorical(y_test)\n",
        "    \n",
        "  x_ = keras.Input(x_train.shape[-3:], name='input_height_width')  # remove depth\n",
        "  y_ = Lenet_like(width, depth, nb_classes)(x_)\n",
        "\n",
        "    # create model\n",
        "  model = keras.Model(inputs = x_, outputs = y_)\n",
        "\n",
        "    # compile model to monitor the accuracy of the model\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # fit model\n",
        "  model.fit(x_train, y_train, epochs=12, batch_size=128, verbose=1, validation_data=(x_test, y_test))\n",
        "\n",
        "    # evaluate the model\n",
        "  scores = model.evaluate(x_test, y_test, verbose=1)  # verbose: 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
        "\n",
        "  print('Test loss:', scores[0])\n",
        "  print('Test accuracy:', scores[1])\n",
        "\n",
        "  model_name = \"keras\"+dataset\n",
        "  if not os.path.isdir(save_dir):\n",
        "      os.makedirs(save_dir)\n",
        "  model_path = os.path.join(save_dir, model_name)\n",
        "  model.save(model_path)\n",
        "\n",
        "  return model, (x_train, x_test), (y_train, y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIJ9JH3x3VXU",
        "colab_type": "text"
      },
      "source": [
        "## 3- Create a fitting function\n",
        "\n",
        "I want a function \"fit_model_on\" with the following arguments:\n",
        "\n",
        "- dataset: str name of the dataset\n",
        "- epochs: number of times you fit on the entire training set\n",
        "- batch_size: number of images to average gradient on\n",
        "\n",
        "The function must create a Lenet model and fit it following these parameters.\n",
        "The function should:\n",
        "\n",
        "- fit the model, obviously\n",
        "- store the model architecture in a .json file in your output directory\n",
        "- store the model's weights in a .h5 file in your output directory\n",
        "- store the fitting metrics loss, validation_loss, accuracy, validation_accuracy under the form of a plot exported in a png file.\n",
        "\n",
        "Run your fitting function of course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl4UE4qc9Fzq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model_on(dataset, depth=1, epochs=10, batch_size=128):\n",
        "    model, (x_train, x_test), (y_train, y_test) = make_lenet_model(dataset, output=10)\n",
        "    \n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size = batch_size,\n",
        "              epochs = epochs,\n",
        "              verbose = 1,\n",
        "              validation_data = (x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy2vraX-_tvn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "480ccfb7-70cc-464c-84e0-7d1d38e43721"
      },
      "source": [
        "fit_model_on(mnist, depth=1, epochs=10, batch_size=128)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-6e309d33c66a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_model_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-a2342990e046>\u001b[0m in \u001b[0;36mfit_model_on\u001b[0;34m(dataset, depth, epochs, batch_size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfit_model_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_lenet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     model.fit(x_train, y_train,\n\u001b[1;32m      5\u001b[0m               \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-df3c845bd123>\u001b[0m in \u001b[0;36mmake_lenet_model\u001b[0;34m(dataset, output)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_height_width'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# removed depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLenet_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'width' referenced before assignment"
          ]
        }
      ]
    }
  ]
}